
# lora
use_lora = False
# sampling
use_sampling = False
sample_size = 160
# training
early_stopping_patience = 3
log_steps = 250
# model
model_name = 'bert-base-uncased'
# save directory
root_save_dir = './results'
save_checkpoint_limit = 2

# seed
data_seed = 42
